{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-benAmmar/PPP_Essai/blob/main/Emotion_Classification_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQeuv-BBiYGJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!pip install keras-tuner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Classifier"
      ],
      "metadata": {
        "id": "KMUwHvxGXtPg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxq_DXcRjmLm"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv('/content/concatenated_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0JaOMW1nFXH"
      },
      "outputs": [],
      "source": [
        "data=data.loc[:,['sentiment','content']]\n",
        "copy=data.copy()\n",
        "data['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRC1n3a6jrPf"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "-AVYZMu06itt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = data[data.duplicated() == True].index\n",
        "data.drop(index, axis = 0, inplace = True)\n",
        "data.reset_index(inplace=True, drop = True)"
      ],
      "metadata": {
        "id": "urV--N8463RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "SzTrIdrq7Fdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[data['content'].duplicated() == True]\n"
      ],
      "metadata": {
        "id": "HzwYQ9VD7LWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = data[data['content'].duplicated() == True].index\n",
        "data.drop(index, axis = 0, inplace = True)\n",
        "data.reset_index(inplace=True, drop = True)"
      ],
      "metadata": {
        "id": "opY9kNdF8nF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBC2QYTGOh7O"
      },
      "source": [
        "#  Data Cleaning and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD7W8r0Gjzhc"
      },
      "outputs": [],
      "source": [
        "stop_words=stopwords.words(\"english\")\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatization(text):\n",
        "    lemmatizer= WordNetLemmatizer()\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    text=[lemmatizer.lemmatize(y) for y in text]\n",
        "    \n",
        "    return \" \" .join(text)\n",
        "\n",
        "def remove_stop_words(text):\n",
        "\n",
        "    Text=[i for i in str(text).split() if i not in stop_words]\n",
        "    return \" \".join(Text)\n",
        "\n",
        "def Removing_numbers(text):\n",
        "    text=''.join([i for i in text if not i.isdigit()])\n",
        "    return text\n",
        "\n",
        "def lower_case(text):\n",
        "    \n",
        "    text = text.split()\n",
        "\n",
        "    text=[y.lower() for y in text]\n",
        "    \n",
        "    return \" \" .join(text)\n",
        "\n",
        "def Removing_punctuations(text):\n",
        "    ## Remove punctuations\n",
        "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
        "    text = text.replace('؛',\"\", )\n",
        "    \n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text =  \" \".join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "def Removing_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def remove_small_sentences(df):\n",
        "    for i in range(len(df)):\n",
        "        if len(df.text.iloc[i].split()) < 3:\n",
        "            df.text.iloc[i] = np.nan\n",
        "            \n",
        "def normalize_text(data):\n",
        "    data[\"content\"]=data[\"content\"].apply(lambda text : Removing_urls(text))\n",
        "    data[\"content\"]=data[\"content\"].apply(lambda text : lower_case(text))\n",
        "    data[\"content\"]=data[\"content\"].apply(lambda text : remove_stop_words(text))\n",
        "    data[\"content\"]=data[\"content\"].apply(lambda text : Removing_numbers(text))\n",
        "    data[\"content\"]=data[\"content\"].apply(lambda text : Removing_punctuations(text)) \n",
        "    data[\"content\"]=data[\"content\"].apply(lambda text : lemmatization(text))\n",
        "    return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guZ_j3FYYmnx"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdUs7s6AlmqF"
      },
      "outputs": [],
      "source": [
        "data=normalize_text(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRYqhejV2VDO"
      },
      "outputs": [],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obmfsKq3l1Ka"
      },
      "outputs": [],
      "source": [
        "data['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbm4JW5am0a-"
      },
      "source": [
        "\n",
        "# LABEL ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPI0IimxmXXs"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "data['sentiment_label']=le.fit_transform(data['sentiment'])\n",
        "data['sentiment_label'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCeuQgbpsrcB"
      },
      "outputs": [],
      "source": [
        "max_length = 0\n",
        "for sentence in data['content']:\n",
        "    length = len(sentence.split())\n",
        "    if length > max_length:\n",
        "        max_length = length\n",
        "max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split"
      ],
      "metadata": {
        "id": "F0fFzKEsYvmo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8ytDDKMjgrW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X=data['content']\n",
        "y=data['sentiment_label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feauture Engineering"
      ],
      "metadata": {
        "id": "5NT5J0WaY-TE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuC_QjsXrVJe"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(X_train)\n",
        "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
        "X_test = word_tokenizer.texts_to_sequences(X_test) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIhAU7RRjDfH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 35\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84wV7NP6m6PN"
      },
      "outputs": [],
      "source": [
        "vocab_length = len(word_tokenizer.word_index) + 1\n",
        "vocab_length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojCPcqZ4yA2F"
      },
      "outputs": [],
      "source": [
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    try:\n",
        "      vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "      embeddings_dictionary[word] = vector_dimensions\n",
        "    except ValueError:\n",
        "      pass\n",
        "\n",
        "glove_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g5R9nss-Hcm"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = zeros((vocab_length, 100))\n",
        "for word, index in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    try:\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[index] = embedding_vector\n",
        "    except ValueError:\n",
        "      print(embedding_vector,word,index)\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSRLZjkjvyx7"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,Flatten,Dense,LSTM,Lambda\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Bidirectional, Dropout\n",
        "from keras import regularizers\n",
        "\n",
        "y_one_hot = to_categorical(y_train, num_classes=6)\n",
        "y_one_hot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_one_hot"
      ],
      "metadata": {
        "id": "JFOw1pY4V648"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building Phase\n",
        "\n"
      ],
      "metadata": {
        "id": "IT8ZIDh9G9Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_length, 100, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))\n",
        "model.add(Bidirectional(LSTM(256, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "                                                                                         \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.005), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-mYjKOiJ4dTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA7JpPXnvy_p"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = model.fit(X_train,\n",
        "                    y_one_hot,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1,\n",
        "                    batch_size=256,\n",
        "                    epochs=6\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_train = np.array(y_train)"
      ],
      "metadata": {
        "id": "ft1F5PXQcIpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "JR3XLZV7cV__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        " \n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "MDr9Ru1XlxhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n"
      ],
      "metadata": {
        "id": "m1pvORC5sfoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "y_pred"
      ],
      "metadata": {
        "id": "HtzfyfiLRKyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [list(i).index(max(i)) for i in y_pred]\n"
      ],
      "metadata": {
        "id": "a_iB6O8kl9vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(y_pred)"
      ],
      "metadata": {
        "id": "vWbxd84er8V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_list=list(y_test)"
      ],
      "metadata": {
        "id": "fKTzMCT_onu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "LvGZmmadk5Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Metrics\n"
      ],
      "metadata": {
        "id": "bBRVT5vhZhME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "e9UAtOH3nSpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Generate classification report\n",
        "\n",
        "# Extract precision, recall, and F1-score from the report\n",
        "classes = list(report.keys())[:-3]\n",
        "custom_labels = ['Anger', 'Fear', 'Joy', 'Love', 'Sadness', 'Surprise']\n",
        "precision = [report[class_name]['precision'] for class_name in classes]\n",
        "recall = [report[class_name]['recall'] for class_name in classes]\n",
        "f1_score = [report[class_name]['f1-score'] for class_name in classes]\n",
        "\n",
        "# Plotting the results\n",
        "x = np.arange(len(classes))\n",
        "width = 0.3\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width, precision, width, label='Precision')\n",
        "rects2 = ax.bar(x, recall, width, label='Recall')\n",
        "rects3 = ax.bar(x + width, f1_score, width, label='F1-Score')\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('LSTM : Classification Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(custom_labels)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OvCIINjYI2PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "Xr-QEfrKZqy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_sentence(sentence):\n",
        "    sentence= lower_case(sentence)\n",
        "    sentence= remove_stop_words(sentence)\n",
        "    sentence= Removing_numbers(sentence)\n",
        "    sentence= Removing_punctuations(sentence)\n",
        "    sentence= Removing_urls(sentence)\n",
        "    sentence= lemmatization(sentence)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "L1mVxipLVJDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "            \"i'm really scared\",\n",
        "            \"oh congratulations !\",\n",
        "            \"I can't do it, I'm not ready to lose anything, just leave me alone\",\n",
        "            \"i am really sad\"\n",
        "            ]\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "    sentence = normalized_sentence(sentence)\n",
        "    sentence = word_tokenizer.texts_to_sequences([sentence])\n",
        "    sentence = pad_sequences(sentence, maxlen=35, truncating='pre')\n",
        "    result = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\n",
        "    proba =  np.max(model.predict(sentence))\n",
        "    print(f\"{result} : {proba}\\n\\n\")"
      ],
      "metadata": {
        "id": "D79MEk7HMW0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#i guess it is stupid Hahahah"
      ],
      "metadata": {
        "id": "vaQ2m96pFaxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8l5zKK-gGlmV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}